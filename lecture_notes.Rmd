---
title: "lecture_notes"
author: "Nathan Bracken"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(here, data.table, dplyr)
```

# EC 425 Notes

Videos to watch
  - Raj Chetty Full    
  - long lecture amy finkelstein
  - 10 page reading

# Hal Varian, chief economist at google 
- We want to take data and make it more interpretable with regards to the world.

# The economist says that the most valuable resource in the world is data 
- We are interested in understanding if we want to spend money to increase education
- We want our spending to translate to some sort of outcome 
- depending on who you work for, you will have different priorities between causal inference and prediction

# Thinking about comparisons
- Units will be a priority for udnerstanding
- The difficult part is finding people or controls that are similar to the others
  - Randomizing the assignment of treatment

# Key Skills
- Discern between correllation and causation
- Identify which problems require prediction and which problems require causal inference
  - Critical thinking
  - Methods
  - Communication

Orient towards applied practitioners, emphasize on research design

# Why Regression?
All models are wrong, but some are useful.

# All modeling exercizes can be broadly categorized into one of two classes. 
1. Regression, continous outcome
  - conditional expectation of Y given X.
  - we are estimating the conoditional expectation function

# Why the conditionoal expectation function
  - The $E[\epsilon | X] = 0$ 
  -  The $E[Y | X]$ minimizes the mean squared error
     - function of X, 

# We want to find a line that minimizes the errors to the averages
    - orthog projections

# Why do we just take the conditional mean
   - who is this average person? why do we focus on conditional mean? 
    - we are focusing heavily on the lienar approximation of the conditional mean function
    - heavily penalizes the observations of outliers     
  - it is a useful baseline case
    - roger Koenker - every parameter woould like to grow up to be a distribution
    
# In class example 
```{r}
fm_df = here("data", "cleaned_Freddie_Mac.Rdata") |> load()

# Keep one record per loan
loan_data <- orig_svcg %>% 
filter(first_record == TRUE) 
# Remove the orig_svcg object 
rm(orig_svcg)

```
- code is available in the slides

```{r}
loan_data %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1/5) +
geom_smooth(method = "lm")
```

```{r}
loan_data %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1/5) +
geom_smooth(method = "lm")
```

# Adding Bins
```{r}
# Add fico_bin to loan_data
loan_data <- loan_data %>% 
mutate(fico_bin = cut(fico, breaks=quantile(fico, probs = seq(0, 1, by = 0.05), na.rm = 
TRUE)))
# Is the fico_bin variable a factor variable?
is.factor(loan_data$fico_bin)
```

# Checking that then umber of observations in each bin are almost equal
```{r}
# Tabulate the number of observations in each fico_bin group
table(loan_data$fico_bin)
```

```{r}
loan_data %>% 
group_by(fico_bin) %>% 
summarise(fico = mean(fico), int_rt = mean(int_rt)) %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1) +
geom_smooth(method = "lm", se = FALSE)
```

```{r}
loan_data %>% 
group_by(fico_bin) %>% 
summarise(fico = mean(fico), int_rt = mean(int_rt)) %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1) +
geom_smooth(method = "lm", se = FALSE)
```

```{r}
# Estimate the linear model
lm_1 <- lm(int_rt ~ 1, data = loan_data)
# Summarize the results of the linear regression
summary(lm_1)
```

# Regression Coefficient equaling exactly the group mean
```{r}
all.equal(lm_1$coefficients, mean(loan_data$int_rt), check.names = FALSE) %>% 
stopifnot()
```


```{r}
# Estimate the linear model
lm_bin_0 <- lm(int_rt ~ 0 + fico_bin, data = loan_data)
# Summarize the results of the linear regression
summary(lm_bin_0)
```

# Binning regression for bin 1
```{r}
# Estimate the linear model
lm_bin_1 <- lm(int_rt ~ 1 + fico_bin, data = loan_data)
# Summarize the results of the linear regression
summary(lm_bin_1)
```

Without intercept show for each bins
With intercept shows the means relative to the intercept

# Why do we want to distinguish between the two? 
Even though we are looking at the exact same information, the standard errors and P-Values are different
Our null hypothesis for each coefficient is equal to 0

We aren't really interested if the coefficient is different.
  - We are interested in if people with better fico scores get better deals
  - we can see that with the intercept that this is true 
  - this process saves some time of doing addition and subtraction

How does binning help?
  - if you are generally interested in fico scores you do not have to bin
  - binning is a visualization tool
  - some people call the lowest bin the omitted category

# Summarizing 
  - linear regression with categorical variables can be used to calculate group means 
  
# Multiple Lienar Regression
  - A little bit more complicated
  - we Have a Y outcome variable and an array of explanatory variables
    - Why?
      - Prediction
      - Inference we are interested in confounders

# Outcome vs Single Predictor

```{r}
library(tidyverse)
# Make data directory
dir.create("data_housing")
# Download data
"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data" %>% 
download.file("data_housing/housing.data")
# Download data dictionary
"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names" %>% 
download.file("data_housing/housing.names")
# Variable names from housing.names
variables <- c("CRIM", "ZN", "INDUS",  "CHAS", "NOX", "RM", "AGE", 
"DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV")
# Read in the data
housing_data <- read_table("data_housing/housing.data", col_names = variables)
```

```{r}
require(stargazer) 
# Simple linear regression
lm_dis <- lm(MEDV ~ DIS, data = housing_data)
# Show regression output using stargazer function
stargazer(lm_dis, type = "text")
```
We can calculate the test statistic as 1.092/.188 -> coefficient / SE.  as N is increasingly large, the the distribution is approximately normal
   - we find the errors of the tip and tail of the distribution

We can also find a confidence interval 

a standard error tells us how noisy the estimate is -> confidence interval can be calculated
  - what this means is that a coefficient can intuitively be the upper or lower bound of the confidence interval
  - two models that have completely different answers
     - we cannot just look at the p values -> we have to look at the distribution of the estimation
     - there can be a substantial overlap between the two parameters
     
```{r}
lm_ses <- lm(MEDV ~ DIS + TAX + NOX + LSTAT, data = housing_data)

lm_nox <- lm(MEDV ~ DIS + TAX + NOX, data = housing_data) 

lm_tax <- lm(MEDV ~ DIS + TAX, data = housing_data)
```

You can control for a lot of things, but you can never understand what you cannot control for

Practice Example
produce a bin scatter plot of the relationship between median and distance

Upward Mobility in the United States
- What is your chance of climbing up the income distribution
  - The percentage of children earning more than their parents

# Why is income mobility difficult to model?
- It is hard to model macroeconomic trends
- we do not know the differences in macroeconomic trends
  - parents household incomes, average income reported on form 1040

- Looking at parents rank in income distribution     
  - if you are born in a rich environment you are much more likely to earn more than your parents
  - they found a tremendous amount of variation city to city
  - these are just the summary stats
  
- What the actual factors that caused a higher probability?   
  - is the city great for the kids or are people that grew up there different people?   
  - lets look at families that move and at what age?    
  - reasons for variation
     - sorting: people willing and able to move
     - causal effects
     
# Lecture 3 Apr 5

## Equality of Opportunity Project
### Data
  Tax return data from the IRS    
  How much their kids are earning from their early 30s
### Projects
  1. The fading American Dream
    Descriptive paper documenting these facts - Science
    Time variation
  2. Quarterly Journal Of Economics "Where is the land of Opportunities?"
    They do a measure of parent to kid income probability for different areas
    Geographic Variation
  3. QJE "Childhood Exposure Effects"
    What is the causal effect of a kid moving from Atlanta to salt lake city
    Causal effects
    At the end of this paper they have a map that is not just descriptive information    
    Have a map of the causal effect of a county on income
  4. QJE "Lost Einstein"    
    Using innovation/patent data to measure income
  5. Working Paper not published yet in Economic Tracker
    Talked to 20 companies that have a real time sense of where people are spending their money
    Compiled the data there
    Can see how much people are spending
    
### Question they are asking
Measuring economic growth
Innovation can be measured pretty precisely with data   
Big Data to study who becomes an inventor in the US

They acknowledge that innovation is very determined by family environment. 

They did this after they claimed that the american dream has faded but it still exists. 
90% to 50%

I feel like it is sort of misleading. I think we are going to get into it a bit here but it still isnt great.
They have distance measures!!

### Discussion about techniques and theory
1. Correlation vs. Causation     
Machine Learning: using data to predict some variable as a function of other variables
   All we care about is the Y hat
Causal Inference: We now care about the Betas
  how does y change with a change of x
  
Problem Analysis:
Some things simply require good prediction
Recommendation systems
Demand
Potential customers

Manipulation of Environment - we care about the betas of what we evaluate

The Causal Question
  does college cause individuals to earn more
  possibly yes possibly no
  
What is the challenge? 
  Individuals who finish college differ in many respects from individuals that do not finish college
  Comparisons must be held under ceteris paribus - holding all else equal

Business and Policy Analysis
  what does it mean for comparisons between groups to have a cuasl interpretation
  what are the basic analytical tools to use data to measure causal connections
    Randomized experiments
      Randomly splitting observations into a control and treatment sample
      Golden standard for causal inference
      not always possible or ethical
    Regression Discontinuity
      Third quarter of 2006 vs fouorth quarter of 2006
    DiffnDiff
      Change in one group compared to a change in another group over time
    IV
All of these things are implementable via simple ols
How do we look at data in specific ways

Randomized Assignment
  this is our first tool of analysis
  Experimental random assignment
  a benchmark for judging results versus other experiments
  "Why is not okay to just compare two different groups?"
  
  Health Insurance Context
    ACA requires people to purchase health insurance
      there is a tax penalty for people who do not purchase it
    THE QUESTION: Does health insurance actually improve your health?
      many people believe that this is true
    Ceteris Paribus: Requires us to compare the health of people with and without insurance
       We cannot observe counterfactuals for individuals
       Outcome - a measure that we are interested in studying
       Treatment group: individuals receiving the treatment
       Control Group: individuals not receiving the treatment
        A good control group describes the fate of the treated group if they had not been treated
        We want to split groups into treatment and controls and evaluate what a group would do without the treatment

How do we go about answering the question of the effects of health insurance on health?
  NHIS annual survey
    create an index 1-5 to capture health status     
    the index will be our outcome measure
    you do not want to observe people who already have health insurance 
      this group is innately different than people who do not have insurance

Formal Definitions
  $D_{i} \in {0,1}$ 
  $D_{i} = 1 \rightarrow$ unit i is treated
  $D_{i} = 0 \rightarrow$ unit i is not treated
  $Y_{i}(D_{i}) =$ outcome for unit i if treatment status is $D_{i}$


The fundamental problem of causal inference
The individual treatment effect of treatment is the difference, but it is impossible to say what the effect of the drug is on the patient "we cannot observe the counter factual"
  With two math and stats tools
    Law of Large Numbers
    Randomized assignment

^Before Tabby slides

Rather than measuring the impacts for each individual, we can estimate the average treatment effect
  We have lots of tabby cats in this world we can estimate the ATE -> the average effect of treatment across an entire sample 
  We will talk about the technical stuff on thursday

  
2. Randomized Exp: Conceptual Framework
 $D_{i} \in {0,1} =$treatment indicator for unit i.
 $D_{i} = 1$ treatment
 $D_{i} = 0$ non treatment
 
 We are interested in how effective $D_{i}$ is
 
 $Y_{i}(D_{i}) =$ outcome for unit i if treatment status is $D_{i}$
 $Y_{i}(1)$ if i is treated
 $Y_{i}(0)$ if i is not treated

The difference in outcome Y when unit i receives treatment relative to the same unit without treatment.

$\tau_{i} = Y_{i}(1) - Y_{i}(0)$

Scientists generally agree it is impossible to estimate the treatment effects for each individual.

If you have:
LLN 
Random assignments

You can say something about the effects of the drug.
Having a lot of data isnt always enough.

Individual treatment effect question:
What is the causal effect of health insurance on Tabby 1's health and tabby 2's health?
Taking the differences for each tabby, you can estimate the effects for each of the cats.

# Average Treatment effects
${\tau_{i}}^N_{i=1}$

$\tau^{ATE} = E[Y_{i}(1)] - E[Y_{i}(0)]$

We have cheated a little bit because we have changed the question. 
The treatment effect does not come without a cost. 
  You get to see what happens on average.
  You do not really get to see what happens for some people.
  We are sometimes interested in very specific narrowly defined groups.
    ask about this? 
    What about economic behavior?   
    What defines narrowly defined groups for economic behavior?
    Conditional classification?

There are two things necessary for average treatment effects
  - Law of large numbers
    When we want to know the expected value of treatment
    LLN - If you have a sample -> the sample average is a proxy for the population average.
  - Random assignment 
    You still do not observe the average for treatment and the non treatment average simultaneously

Questions:
What if we use observational data to calculate the difference in outcome for those with $D_{i} = 0$ vs $D_{i} = 1$. 
 How bad is this for approximating the average treatment effect. 

Why is random assignment much better than observational data?

What do we want:
$\tau^{ATE} = E[Y_{i}(1)] - E[Y_{i}(0)]$

(Naive) group means differences
   An appealing estimate of ATE.
   $E[Y_{i}(1)|D_{i} = 1] - E[Y_{i}(0)|D_{i} = 0]$
   Is there a specific reason that notation has been changed to $Avg_{n}[]$ from $E[]$.
   -> Expected value is before you know. Average is after you know.
    
What you observe in the treatment group may not be what you see in the population.

# Who buys insurance?
There seems to be some sort of selection bias here.
Fun cats purchase health insurance because they know they might need health insurance in the future.

Naive estimator of the causal effect 
$= Avg_{n}[Y_{i}(1)|D_{i} = 1] - Avg_{n}[Y_{i}(0)|D_{i} = 0]$

Difference in group means: 
You can add and subtract counter factual outcomes for the treated cats.
Among the cats that have health insurance, we are adding and subtracting what would have been their outcome if they are not insured.
  Specifically:
$Avg_{n}[Y_{i}(0)|D_{i} = 1]$ and $Avg_{n}[Y_{i}(0)|D_{i} = 1$

What is the issue?
  We only get the top or bottom rows.
    Even without the treatment, treated tabbys outcome would have been different than untreated Tabbys.
    
  In other words, there are reasons why treated tabbys are treated in the first place.
  Tabbys select into treatement according to some characterisitcs
  
  Depending on the magnitude of selection bias relative to the real rtreatement effect, this can mess up your biases significantly. 
  Example 
  The last digit of your social security number
    This should really be pretty random.
    If you compare people based off of this classifier
    Mean difference does not have to be a bad estimator for causal effects.
  
  Differencing group means still tells us that insured tabbys tend to be more fun than uninsured tabbys.    
    we cannot say that insurance causes tabbies to be fun
    this is essentially what correlation is not causation means
  
  It is not uncommon in causal analysis that running the treatment with the same data you get opposite signs
  
# We are now going to try and think about randomized experiments vs naive means
  Technical detail
  We said that the difference in group means is 
  = $Avg_{n}[Y_{i}(1)|D_{i} = 1] - Avg_{n}[Y_{i}(0)|D_{i} = 1] + Avg_{n}[Y_{i}(0)|D_{i} = 1] - Avg_{n}[Y_{i}(0)|D_{i} = 0]$
  = Average treatment effect on the treated + selection bias
What is the ATET
  - the impact of the treatment effect on the treated units

Under homogenous treatment effects
  Supposing a drug works exactly the same for every individual
  we assume that ATET = ATE, generally not correct

The Power of large sampel and random assignment.
  When treatment is assigned randomly and largely enough, bias disappears
  
  Randomize over a lot of cats 
    We should observe that the insured group and the uninsured group have the same amount and type of cats
    We are basically creating two parallel universes
    we see the mena of the top and the mean of the bottom
  Why we need law of large numbers
    If we have 16 cats and we randomize, there is a higher chance of having imbalancees in our parallel universes
  
  Once you have randomly assigned, what is the estimator?
    All of a sudden, the naiive mean becomes the best estimate of the average treatment effect.
    Causal estimator $= Avg_{n}[Y_{i}(1)|D_{i} = 1] - Avg_{n}[Y_{i}(0)|D_{i} = 0]$
    Doing the same thing on cats that you randomly assign to, this is a great estimation if you assign appropriately.
  
  Why does this work? 
    Doing the add and subtract and add the counter factual outcome
    $Avg_{n}[Y_{i}(0)|D_{i} = 1]$
    We run into these calculations again:
    = $Avg_{n}[Y_{i}(1)|D_{i} = 1] - Avg_{n}[Y_{i}(0)|D_{i} = 1] + Avg_{n}[Y_{i}(0)|D_{i} = 1] - Avg_{n}[Y_{i}(0)|D_{i} = 0]$
    = Average treatment effect on the treated + selection bias
    Here the selection bias = 0
    this is because the treatment is randomly assigned.
    Untreated tabbies serve as a valid control group.
    With a large sample and random assignment, correlation is causation.
  
  How do we find ways to approximate this?
    There are some types of assignment that happen in the real world that mirror this assignment nearly as well as an experiment would do.
    Within a month looking at eugene
       within a month on the same day of the week
       what is the probability, lets compare people's mood and performance
  
  Review 1: Mathematical Expectation
    The expectation of a variable $Y_{i} is E[Y_{i}]$
      Expectation is the average of a population 
      Average is the average of sample.
    
  The conditional expectation of a variable $Y_{i}$
  Law of large numbers:
    Sample average versus population average
    the sample average is given by $Avgn[Y_{i}]$ = sum of y_{i} divided by n.
  
  Law of large numbers, sufficiently sampling a large number of individual, then the average of Y_{i} is equal to E{Y_{i}}
  When treatment is assigned randomly and samples are sufficiently large, selection bias disappears
    Selection bias $= Avg_{n}[Y_{0i}|T_{i}  = 1] - Avg_{n}[Y_{0i}|T_{i} = 0]$
    By the law of large numbers the average is approximately the expectation and the treatment and non-treatment effects are the same.
    
  Causal inference with regressions
    Regression analysis of randomized experiment
    We are going to look at a semitheoretical framework to a regression
    
  Start by writing the observed outcomes as a function of potential outcomes
    Let
    
  $Y_{i} = D_{i}Y_{i}(1) + (1- D_{i})Y_{i}(0)$
  = D_{i}Y_{i}(1) + Y_{i}(0) - D_{i}Y_{i}(0)
  = Y_{i}(0) + .....
  
If we run a regression on treatment indicators, will the regression coefficient give us the correct treatment effect? 
  Suppose we have an error term that is independent from D_{i}, we then have an unbiased estimator

What does randomization give me? 
  No selection bias
  a random component of Y_{i}(0)
  suppose D_{i} is a random treatment indicator, regressing Y_{i} on D_{i} to give me an estimate of the causal effect.

We will talk a little bit about adding controls
  This is useful because things may be unbalanced between the treatment group
  We have already randomized so we do not need to worry about the balance of Di between the groups
  Never control for things that will be effected by the treatment itself
    we are introducing something into our variable that is a function of what we are interested in
    this is a bad control
    something that can be changed by the treatment
    there are a few more slides on bad controls
  
Here is the intuition
What we want to know is that 
For the entire population was the effect of health insurance
We end up saying what is the 
  
Real health insurance experiments
  US national health expenditures
  US health spending over time has increased from 4.5% too 18%
  
  By 2040
    total health spending is 34% of budget
    Medicare and medicaid is 15% of GDP
  
  Just because spending is high does not mean that the consumption is not worth it
    large health improvemnts since 1950
    innovations in medical technology and health care
    Canadians spend much less on healthcare
    across different counties in the united states, spending on healthcare can vary dramatically
    
  There is a ton of research in health economics   
    30-50% of spending is wasted
  
  Does health insurance increase or decrease costs?    
    maybe insurance provides more efficient care, avoiding the ER
  
  Dooes health insurance improve health?
    - not machine learning because we do not care abut what proportion of health is explained by health insurance
  
  Health insurance experiments
    - does health improve?    
    - does health care utilization and spending increase?
  
  Random Health insurance experiment:
    what do people actually do in the field
    how do we implement regressions on empiracle data
  
  RAND
    large scale multi year experiment
    individuals were randomized into different insurance plans
    QUESTIONS:
      How does cost sharing affect the use of health services? 
      How does cost sharing affect the quality of care> 
      What are the health consequences?
      
  DIAGRAM:
    Patients are organized into four tiers
    Catastrophic plan
      patient maus most of the costs
      1000 dollars or proportion of income
    Deductible plan 
      patient paid 95% of costs
      max spending 150
        450$ per family
    coinsurance plan 
      25% or 50% of costs
      capping at 1000$ of costs
    free care plan
    
  What is the control group and what is the treatment group
    Control group is the catastrophic plan
    Treatment is any insurance
      adding treatments muddies the statistical power but muddies the interpretation
        the different structures of plans makes a difference
    Depending on what people want to tease out:   
      you generally have lots of arms
        for the baseline, when you want to show that the treatment matters
    you need to show that you have randomized
      if your randomization works
        all of the characteristics must be balanced 
        in fact, all of these characteristics balanced out nicely
    What do you need for the experiment to work
      E[Y_{i}|Treatment]=E[Y_{i}(0)|Control] 
        You can show that all of the inputs to Y_{i} are balanced out.
        How do you observe that other things are balanced out that you do not observe
    Why are we happy that treatment is balanced? 
      The confidence intervals contain the true average
      We believe that the treatment is balanced because that standard errors are small compared to the mean of the group
      
    
    
      

  Lets randomize people's health by giving them less money!
    - what is the sample?
    - did people volunteer?
    - i dont think that this is even remotely ethical
      - does depend on sample though
      - someone could have died for the sake of quantifying the returns of insurance to health
      - not even a perfect measurement but an on average 
      - im willing to bet that the people in the catastrophic group had more severe negative health outcomes
        - "health crises"
        - medicine and health is preventative, super correlated with habits/environment
        - long run care rather than reacting to bad stuff
        - so maybe it was cheaper overall for people providing insurance because the sum of treatment is less
        - expensive for people's health
        - AND THEN THE BALANCING COULD FAIL 
          - we dont learn anything and people get sick!!!
          - so not only are we spending money to evaluate how healthy people get from spending money
            - we are now spending money to get people sick and then not learning how much of a difference it makes
       
       
   - maybe a meta experiment that could be used to compare the health of people in other countries that have public health programs vs ones that do not?
   
   
Oregon Health Plan Experiment (OHP)
- they cannot give free medicaid to everyone, they have people sign up for slots and they arranged them randomly

Finkelstein Becker Friedman
  - there is a large range of reported effectiveness for medicaid
  - useless piece of plastic vs life saving

Research
  - we do not want to rely on one or two extreme examples from newspapers

What does the paper look at OHP
  - Ever on medicaid - in the rand health experiment

Adjusting for non-compliance
   - 50% of the treatment group won a lottery actually went on to obtain health insurance
   - the true impact is known as the average treatment effect on the treated

Generally ITT
   - divide the difference - slide 50 
   - it does not need to be the true effect of being treated
   - what is the effect of the medicaid program
  


