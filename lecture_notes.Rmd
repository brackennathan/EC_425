---
title: "lecture_notes"
author: "Nathan Bracken"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(here, data.table, dplyr)
```

# EC 425 Notes

# Hal Varian, chief economist at google 
- We want to take data and make it more interpretable with regards to the world.

# The economist says that the most valuable resource in the world is data 
- We are interested in understanding if we want to spend money to increase education
- We want our spending to translate to some sort of outcome 
- depending on who you work for, you will have different priorities between causal inference and prediction

# Thinking about comparisons
- Units will be a priority for udnerstanding
- The difficult part is finding people or controls that are similar to the others
  - Randomizing the assignment of treatment

# Key Skills
- Discern between correllation and causation
- Identify which problems require prediction and which problems require causal inference
  - Critical thinking
  - Methods
  - Communication

Orient towards applied practitioners, emphasize on research design

# Why Regression?
All models are wrong, but some are useful.

# All modeling exercizes can be broadly categorized into one of two classes. 
1. Regression, continous outcome
  - conditional expectation of Y given X.
  - we are estimating the conoditional expectation function

# Why the conditionoal expectation function
  - The $E[\epsilon | X] = 0$ 
  -  The $E[Y | X]$ minimizes the mean squared error
     - function of X, 

# We want to find a line that minimizes the errors to the averages
    - orthog projections

# Why do we just take the conditional mean
   - who is this average person? why do we focus on conditional mean? 
    - we are focusing heavily on the lienar approximation of the conditional mean function
    - heavily penalizes the observations of outliers     
  - it is a useful baseline case
    - roger Koenker - every parameter woould like to grow up to be a distribution
    
# In class example 
```{r}
fm_df = here("data", "cleaned_Freddie_Mac.Rdata") |> load()

# Keep one record per loan
loan_data <- orig_svcg %>% 
filter(first_record == TRUE) 
# Remove the orig_svcg object 
rm(orig_svcg)

```
- code is available in the slides

```{r}
loan_data %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1/5) +
geom_smooth(method = "lm")
```

```{r}
loan_data %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1/5) +
geom_smooth(method = "lm")
```

# Adding Bins
```{r}
# Add fico_bin to loan_data
loan_data <- loan_data %>% 
mutate(fico_bin = cut(fico, breaks=quantile(fico, probs = seq(0, 1, by = 0.05), na.rm = 
TRUE)))
# Is the fico_bin variable a factor variable?
is.factor(loan_data$fico_bin)
```

# Checking that then umber of observations in each bin are almost equal
```{r}
# Tabulate the number of observations in each fico_bin group
table(loan_data$fico_bin)
```

```{r}
loan_data %>% 
group_by(fico_bin) %>% 
summarise(fico = mean(fico), int_rt = mean(int_rt)) %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1) +
geom_smooth(method = "lm", se = FALSE)
```

```{r}
loan_data %>% 
group_by(fico_bin) %>% 
summarise(fico = mean(fico), int_rt = mean(int_rt)) %>% 
ggplot(aes(x = fico, y = int_rt)) +
geom_point(colour = "firebrick", size = 2, alpha = 1) +
geom_smooth(method = "lm", se = FALSE)
```

```{r}
# Estimate the linear model
lm_1 <- lm(int_rt ~ 1, data = loan_data)
# Summarize the results of the linear regression
summary(lm_1)
```

# Regression Coefficient equaling exactly the group mean
```{r}
all.equal(lm_1$coefficients, mean(loan_data$int_rt), check.names = FALSE) %>% 
stopifnot()
```


```{r}
# Estimate the linear model
lm_bin_0 <- lm(int_rt ~ 0 + fico_bin, data = loan_data)
# Summarize the results of the linear regression
summary(lm_bin_0)
```

# Binning regression for bin 1
```{r}
# Estimate the linear model
lm_bin_1 <- lm(int_rt ~ 1 + fico_bin, data = loan_data)
# Summarize the results of the linear regression
summary(lm_bin_1)
```

Without intercept show for each bins
With intercept shows the means relative to the intercept

# Why do we want to distinguish between the two? 
Even though we are looking at the exact same information, the standard errors and P-Values are different
Our null hypothesis for each coefficient is equal to 0

We aren't really interested if the coefficient is different.
  - We are interested in if people with better fico scores get better deals
  - we can see that with the intercept that this is true 
  - this process saves some time of doing addition and subtraction

How does binning help?
  - if you are generally interested in fico scores you do not have to bin
  - binning is a visualization tool
  - some people call the lowest bin the omitted category

# Summarizing 
  - linear regression with categorical variables can be used to calculate group means 
  
# Multiple Lienar Regression
  - A little bit more complicated
  - we Have a Y outcome variable and an array of explanatory variables
    - Why?
      - Prediction
      - Inference we are interested in confounders

# Outcome vs Single Predictor

```{r}
library(tidyverse)
# Make data directory
dir.create("data_housing")
# Download data
"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data" %>% 
download.file("data_housing/housing.data")
# Download data dictionary
"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names" %>% 
download.file("data_housing/housing.names")
# Variable names from housing.names
variables <- c("CRIM", "ZN", "INDUS",  "CHAS", "NOX", "RM", "AGE", 
"DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV")
# Read in the data
housing_data <- read_table("data_housing/housing.data", col_names = variables)
```

```{r}
require(stargazer) 
# Simple linear regression
lm_dis <- lm(MEDV ~ DIS, data = housing_data)
# Show regression output using stargazer function
stargazer(lm_dis, type = "text")
```
We can calculate the test statistic as 1.092/.188 -> coefficient / SE.  as N is increasingly large, the the distribution is approximately normal
   - we find the errors of the tip and tail of the distribution

We can also find a confidence interval 

a standard error tells us how noisy the estimate is -> confidence interval can be calculated
  - what this means is that a coefficient can intuitively be the upper or lower bound of the confidence interval
  - two models that have completely different answers
     - we cannot just look at the p values -> we have to look at the distribution of the estimation
     - there can be a substantial overlap between the two parameters
     
```{r}
lm_ses <- lm(MEDV ~ DIS + TAX + NOX + LSTAT, data = housing_data)

lm_nox <- lm(MEDV ~ DIS + TAX + NOX, data = housing_data) 

lm_tax <- lm(MEDV ~ DIS + TAX, data = housing_data)
```

You can control for a lot of things, but you can never understand what you cannot control for

Practice Example
produce a bin scatter plot of the relationship between median and distance

Upward Mobility in the United States
- What is your chance of climbing up the income distribution
  - The percentage of children earning more than their parents

# Why is income mobility difficult to model?
- It is hard to model macroeconomic trends
- we do not know the differences in macroeconomic trends
  - parents household incomes, average income reported on form 1040

- Looking at parents rank in income distribution     
  - if you are born in a rich environment you are much more likely to earn more than your parents
  - they found a tremendous amount of variation city to city
  - these are just the summary stats
  
- What the actual factors that caused a higher probability?   
  - is the city great for the kids or are people that grew up there different people?   
  - lets look at families that move and at what age?    
  - reasons for variation
     - sorting: people willing and able to move
     - causal effects
     
# Lecture 3 Apr 5

## Equality of Opportunity Project
### Data
  Tax return data from the IRS    
  How much their kids are earning from their early 30s
### Projects
  1. The fading American Dream
    Descriptive paper documenting these facts - Science
    Time variation
  2. Quarterly Journal Of Economics "Where is the land of Opportunities?"
    They do a measure of parent to kid income probability for different areas
    Geographic Variation
  3. QJE "Childhood Exposure Effects"
    What is the causal effect of a kid moving from Atlanta to salt lake city
    Causal effects
    At the end of this paper they have a map that is not just descriptive information    
    Have a map of the causal effect of a county on income
  4. QJE "Lost Einstein"    
    Using innovation/patent data to measure income
  5. Working Paper not published yet in Economic Tracker
    Talked to 20 companies that have a real time sense of where people are spending their money
    Compiled the data there
    Can see how much people are spending
    
### Question they are asking
Measuring economic growth
Innovation can be measured pretty precisely with data   
Big Data to study who becomes an inventor in the US

They acknowledge that innovation is very determined by family environment. 

They did this after they claimed that the american dream has faded but it still exists. 
90% to 50%

I feel like it is sort of misleading. I think we are going to get into it a bit here but it still isnt great.
They have distance measures!!

### Discussion about techniques and theory
1. Correlation vs. Causation     
Machine Learning: using data to predict some variable as a function of other variables
   All we care about is the Y hat
Causal Inference: We now care about the Betas
  how does y change with a change of x
  
Problem Analysis:
Some things simply require good prediction
Recommendation systems
Demand
Potential customers

Manipulation of Environment - we care about the betas of what we evaluate

The Causal Question
  does college cause individuals to earn more
  possibly yes possibly no
  
What is the challenge? 
  Individuals who finish college differ in many respects from individuals that do not finish college
  Comparisons must be held under ceteris paribus - holding all else equal

Business and Policy Analysis
  what does it mean for comparisons between groups to have a cuasl interpretation
  what are the basic analytical tools to use data to measure causal connections
    Randomized experiments
      Randomly splitting observations into a control and treatment sample
      Golden standard for causal inference
      not always possible or ethical
    Regression Discontinuity
      Third quarter of 2006 vs fouorth quarter of 2006
    DiffnDiff
      Change in one group compared to a change in another group over time
    IV
All of these things are implementable via simple ols
How do we look at data in specific ways

Randomized Assignment
  this is our first tool of analysis
  Experimental random assignment
  a benchmark for judging results versus other experiments
  "Why is not okay to just compare two different groups?"
  
  Health Insurance Context
    ACA requires people to purchase health insurance
      there is a tax penalty for people who do not purchase it
    THE QUESTION: Does health insurance actually improve your health?
      many people believe that this is true
    Ceteris Paribus: Requires us to compare the health of people with and without insurance
       We cannot observe counterfactuals for individuals
       Outcome - a measure that we are interested in studying
       Treatment group: individuals receiving the treatment
       Control Group: individuals not receiving the treatment
        A good control group describes the fate of the treated group if they had not been treated
        We want to split groups into treatment and controls and evaluate what a group would do without the treatment

How do we go about answering the question of the effects of health insurance on health?
  NHIS annual survey
    create an index 1-5 to capture health status     
    the index will be our outcome measure
    you do not want to observe people who already have health insurance 
      this group is innately different than people who do not have insurance

Formal Definitions
  $D_{i} \in {0,1}$ 
  $D_{i} = 1 \rightarrow$ unit i is treated
  $D_{i} = 0 \rightarrow$ unit i is not treated
  $Y_{i}(D_{i}) =$ outcome for unit i if treatment status is $D_{i}$


The fundamental problem of causal inference
The individual treatment effect of treatment is the difference, but it is impossible to say what the effect of the drug is on the patient "we cannot observe the counter factual"
  With two math and stats tools
    Law of Large Numbers
    Randomized assignment

^Before Tabby slides

Rather than measuring the impacts for each individual, we can estimate the average treatment effect
  We have lots of tabby cats in this world we can estimate the ATE -> the average effect of treatement across an entire sample 
  We will talk about the technical stuff on thursday

  
2. Randomized Exp: Conceptual Framework
3. 
